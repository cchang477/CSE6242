{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e0e4e4-d34a-445f-9d10-1bf26d6afd88",
   "metadata": {},
   "source": [
    "code adapted from https://www.kaggle.com/code/lucamassaron/fine-tune-llama-3-for-sentiment-analysis#Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370058d-168d-4d09-8614-e0dd35fc6e56",
   "metadata": {},
   "source": [
    "# Description and contents\n",
    "In this notebook, we\n",
    "1. Process our data for finetuning our sentiment analyzer<br>\n",
    "&nbsp; - We save the processed data to 2_data_dict.pkl<br><br>\n",
    "2. Evaluate the performance of the standard Llama 3.2 LLM as a sentiment analyzer for our reviews<br>\n",
    "&nbsp; - We save the processed data to 2_base_eval.json<br><br>\n",
    "3. Experiment in writing code to finetune a sentiment analyzer for our 'co' rating category<br><br>\n",
    "4. Experiment in writing code to finetune sentiment analyzers for all rating categories<br>\n",
    "&nbsp; - The actual finetuning is performed with ./finetune/3_finetune.py and 3_run_finetune.bat<br>\n",
    "&nbsp; - The batch file loops through each rating category to perform the finetuning; create a shell script if running in a Unix-based environment<br><br>\n",
    "5. Experiment in writing code to evaluate the performance of our finetuned sentiment analyzers<br>\n",
    "&nbsp; - The actual evaluation is performed with ./finetune/3_test_eval.py and 3_run_test_eval.bat<br>\n",
    "&nbsp; - The batch file loops through each rating category to perform the evaluations; create a shell script if running in a Unix-based environment<br><br>\n",
    "6. Process our firm summaries for sentiment prediction with our finetuned models<br>\n",
    "&nbsp; - We save the processed data to 2_summ_text.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80cfaf-4e1c-4f97-a004-29a727828037",
   "metadata": {},
   "source": [
    "Contents:\n",
    "1. [Imports, installs, and setup params](#imports)\n",
    "2. [Load data](#load)\n",
    "3. [Process data](#process)\n",
    "4. [Evaluate base Llama 3.2 performance](#std_eval)\n",
    "5. [Experimental code to finetune for 'co' category](#co_ft)\n",
    "6. [Experimental code to finetune for all categories](#all_ft)\n",
    "7. [Experimental code to evaluate finetuned model performance](#all_eval)\n",
    "8. [Create firm summary dataset for prediction](#createdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45caa1-9d0f-41f3-95ec-38f9b5e0e7bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "# Imports, installs, and setup params <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb3cb723-43b4-44e1-9454-515418f44230",
   "metadata": {},
   "source": [
    "# for torch install, change url depending on cuda version. Here I install torch for cuda 12.4\n",
    "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U transformers==\"4.40.0\"\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U tensorboard\n",
    "!pip install -q -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c167faa8-db41-4045-abab-f4a14c4f9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63d1cb2-165b-476f-924d-da6c3f63bd79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                            AutoTokenizer, \n",
    "                            BitsAndBytesConfig, \n",
    "                            TrainingArguments, \n",
    "                            pipeline, \n",
    "                            logging,\n",
    "                            EarlyStoppingCallback, \n",
    "                            IntervalStrategy)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            classification_report, \n",
    "                            confusion_matrix,\n",
    "                            recall_score, \n",
    "                            precision_score, \n",
    "                            f1_score)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bebc454b-c82f-41c0-9b83-dd71be2a5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d36a28d-1db0-447c-b084-b5b326198295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version 2.5.1+cu124\n",
      "working on cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ac9908-82b2-4b98-88be-b79e9b9debe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80884ff2-769c-44b4-828a-e2365a03e566",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "# Load data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a96d766-bf96-44f3-9b11-3f72288e53cb",
   "metadata": {},
   "source": [
    "with open('1_LLM_finetuning_training_dataset.pkl','rb') as f:\n",
    "    train_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5573b014-4fb9-4153-bb98-a02cafb8d698",
   "metadata": {},
   "source": [
    "train_dict.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f688b91e-b0fa-4a44-ac56-4fef10f06360",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(columns=['category','sentiment','text'])\n",
    "cat_map = dict(zip(train_dict.keys(),['career opportunities',\n",
    "                                     'compensation and benefits',\n",
    "                                     'senior management',\n",
    "                                     'culture and values',\n",
    "                                     'diversity and inclusion',\n",
    "                                      'work life balance']))\n",
    "ratings_map = {1:'terrible',\n",
    "              2:'bad',\n",
    "              3:'neutral',\n",
    "              4:'good',\n",
    "              5:'excellent'}\n",
    "for cat in train_dict.keys():\n",
    "    for rating in range(1,6):\n",
    "        reviews = train_dict[cat][rating]\n",
    "        ratings = [ratings_map[rating]] * len(reviews)\n",
    "        cats = [cat] * len(reviews)\n",
    "        cat_full = [cat_map[cat]] * len(reviews)\n",
    "        df = pd.concat([df,pd.DataFrame(list(zip(cats,cat_full,ratings,reviews)),columns=['category','category_full','sentiment','text'])])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e9df5bc-9d4b-4767-a0cf-5a462059812c",
   "metadata": {},
   "source": [
    "df['stratify']=df.category + ' ' + df.sentiment.astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cf76cf6-1a9d-4ae5-9ea3-bfec778653ee",
   "metadata": {},
   "source": [
    "train, test = train_test_split(df,test_size=0.3,random_state=42,stratify=df.stratify)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f247a06d-d719-46b5-b9c3-f5a1c4093688",
   "metadata": {},
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ad558e-d79d-4c61-aa41-f2ae0ddc788a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Process data <a name=\"process\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ea3dd4-c86a-4466-8e99-0fcc95067492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "On the topic of {data_point[\"category_full\"]}, analyze the sentiment of the company review enclosed in square brackets,\n",
    "determine if it is excellent, good, neutral, bad, or terrible, and return the answer as \n",
    "the corresponding sentiment label \"excellent\" or \"good\" or \"neutral\" or \"bad\" or \"terrible\".\n",
    "\n",
    "[{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "On the topic of {data_point[\"category_full\"]}, analyze the sentiment of the company review enclosed in square brackets,\n",
    "determine if it is excellent, good, neutral, bad, or terrible, and return the answer as \n",
    "the corresponding sentiment label \"excellent\" or \"good\" or \"neutral\" or \"bad\" or \"terrible\".\n",
    "\n",
    "[{data_point[\"text\"]}] = \"\"\".strip()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c336980-136c-4f92-8959-1e35de5f47d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "train['prompt'] = train.apply(generate_prompt,axis=1)\n",
    "test['prompt'] = test.apply(generate_test_prompt,axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d03be141-cd5b-4fa6-b3de-dec1c1791f06",
   "metadata": {},
   "source": [
    "train.prompt.iloc[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5200bc0e-632c-49e0-a756-70ee7b67e2de",
   "metadata": {},
   "source": [
    "data_dict = {'train':{},'test':{}, 'y_true': {}}\n",
    "for cat in train_dict.keys():\n",
    "    data_dict['train'][cat]=Dataset.from_pandas(train[train.category==cat][['prompt']].reset_index(drop=True))\n",
    "    data_dict['test'][cat]=test[test.category==cat][['prompt']].reset_index(drop=True)\n",
    "    data_dict['y_true'][cat]=test[test.category==cat]['sentiment'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "925a58af-ae66-4389-bcb6-797ae943f861",
   "metadata": {},
   "source": [
    "data_dict['train']['co']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "421f064e-3636-4c80-a6dc-d13e7f2e9c35",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open('2_data_dict.pkl','wb') as f:\n",
    "    pickle.dump(data_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c96ab-3c9f-4e82-8483-f7bfc6e250cc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e2f5b17-89dd-4dea-abf9-c534d8006b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_data_dict.pkl','rb') as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db5b1c-6dc6-4857-997f-dee0adf3546a",
   "metadata": {},
   "source": [
    "# Evaluate standard model <a name=\"std_eval\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8488de80-5552-4ce9-b9f7-891f8f77bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b51faa145bc4b6084ce02e8d01c3dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 512 #2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ac5565-fdea-4eb2-a938-caa22d27b652",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    replies = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test.iloc[i][\"prompt\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 3,\n",
    "                        #temperature = 0.7,\n",
    "                        do_sample = False, # equivalent to temperature 0 i.e. deterministic process\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "        replies.append(result[0]['generated_text'])\n",
    "        if \"terrible\" in answer:\n",
    "            y_pred.append(\"terrible\")\n",
    "        elif \"bad\" in answer:\n",
    "            y_pred.append(\"bad\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        elif \"good\" in answer:\n",
    "            y_pred.append(\"good\")\n",
    "        elif \"excellent\" in answer:\n",
    "            y_pred.append(\"excellent\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8536ae22-672a-4f36-a953-9552e81bc223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, verbose=False, print_reports=True):\n",
    "    labels = ['terrible', 'bad', 'neutral', 'good', 'excellent']\n",
    "    mapping = {'terrible':0, 'bad':1, 'neutral':2, 'none':2, 'good':3, 'excellent':4}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "    if verbose==True:\n",
    "        print(f'y_true: {y_true}')\n",
    "        print(f'y_pred: {y_pred}')\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2, 3, 4])\n",
    "    \n",
    "    if print_reports==True:\n",
    "        print(f'Accuracy: {accuracy:.3f}')        \n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')        \n",
    "        print('\\nClassification Report:')\n",
    "        print(class_report)\n",
    "        print('\\nConfusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "    \n",
    "    return class_report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "350765f9-247e-4be2-876d-1a61be4fd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [04:28<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "to_predict = data_dict['test']['co'].iloc[:]\n",
    "y_pred, results = predict(to_predict, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "aed844e6-22d5-4df7-9397-0a302369f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.277\n",
      "Accuracy for label 0: 0.077\n",
      "Accuracy for label 1: 0.513\n",
      "Accuracy for label 2: 0.007\n",
      "Accuracy for label 3: 0.353\n",
      "Accuracy for label 4: 0.433\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.08      0.14       300\n",
      "           1       0.28      0.51      0.36       300\n",
      "           2       0.50      0.01      0.01       300\n",
      "           3       0.29      0.35      0.32       300\n",
      "           4       0.24      0.43      0.31       300\n",
      "\n",
      "    accuracy                           0.28      1500\n",
      "   macro avg       0.39      0.28      0.23      1500\n",
      "weighted avg       0.39      0.28      0.23      1500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 23 182   1  25  69]\n",
      " [  6 154   1  41  98]\n",
      " [  3 100   2  80 115]\n",
      " [  2  61   0 106 131]\n",
      " [  2  50   0 118 130]]\n"
     ]
    }
   ],
   "source": [
    "y_true = data_dict['y_true']['co'][:]\n",
    "evaluate(y_true, y_pred, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1166a3e4-2ac4-4f33-8aa9-ea0a39cef5f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:07<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [04:58<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:03<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:02<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:05<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:05<00:00,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_eval = {}\n",
    "for key in data_dict['train'].keys():\n",
    "    to_predict = data_dict['test'][key].iloc[:]\n",
    "    y_pred, _ = predict(to_predict, model, tokenizer)\n",
    "    y_true = data_dict['y_true'][key][:]\n",
    "    class_report, conf_matrix = evaluate(y_true, y_pred, verbose=False, print_reports=False)\n",
    "    base_eval[key]={'y_true':y_true.to_list(),\n",
    "                     'y_pred':y_pred,\n",
    "                     'cls_report':class_report,\n",
    "                     'conf_matrix':conf_matrix.tolist()}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99ddcc21-9477-477a-9d2d-e57e262f024f",
   "metadata": {},
   "source": [
    "with open('2_base_eval.json','w') as f:\n",
    "    json.dump(base_eval,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685ed577-66e7-41b0-b7e6-e98f34776546",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_base_eval.json','r') as f:\n",
    "    base_eval = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686a679-4b19-4a1f-a62e-e9aaae21bceb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Fine-tuning for 'co' category (experiment) <a name=\"co_ft\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f51210f5-b839-4d4f-9c3e-1c164f595c51",
   "metadata": {
    "tags": []
   },
   "source": [
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c4dc271d-bea0-4bb1-8efb-4d049a84521d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir=\"./trained_weights/co\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a70dc988-d15e-4729-bb44-ceecaf0de45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=5,                       # number of training epochs\n",
    "    per_device_train_batch_size=1,            # batch size per device during training\n",
    "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    #evaluation_strategy=\"steps\",              # save checkpoint every epoch\n",
    "    #load_best_model_at_end = True,\n",
    "    #eval_steps = 25,\n",
    "    #metric_for_best_model = 'accuracy',\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdb2f1dd-e884-4c01-8e96-5fd8b0ca5ca6",
   "metadata": {},
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data_dict['train']['co'],\n",
    "    #eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "915fc919-7c06-47b3-9f18-3ba1b5124ddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd5cfeb4-639c-4de6-addf-36a371ab1482",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8dd57ee2-6929-49cb-adbf-ab7ba2f0df95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1500 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [07:22<00:00,  3.39it/s]\n"
     ]
    }
   ],
   "source": [
    "to_predict = data_dict['test']['co'].iloc[:]\n",
    "y_pred, results = predict(to_predict, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b7095033-71c6-47cf-9c06-6db12b66b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.468\n",
      "Accuracy for label 0: 0.597\n",
      "Accuracy for label 1: 0.507\n",
      "Accuracy for label 2: 0.310\n",
      "Accuracy for label 3: 0.397\n",
      "Accuracy for label 4: 0.530\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.60      0.61       300\n",
      "           1       0.38      0.51      0.44       300\n",
      "           2       0.35      0.31      0.33       300\n",
      "           3       0.40      0.40      0.40       300\n",
      "           4       0.63      0.53      0.57       300\n",
      "\n",
      "    accuracy                           0.47      1500\n",
      "   macro avg       0.48      0.47      0.47      1500\n",
      "weighted avg       0.48      0.47      0.47      1500\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179  89  24   7   1]\n",
      " [ 66 152  57  23   2]\n",
      " [ 32  91  93  63  21]\n",
      " [  2  54  54 119  71]\n",
      " [  4   9  40  88 159]]\n"
     ]
    }
   ],
   "source": [
    "y_true = data_dict['y_true']['co'][:]\n",
    "evaluate(y_true, y_pred, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "2a9aa2cc-3baf-4f41-a4b1-cbd484782740",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame({'prompt': data_dict['test']['co']['prompt'], \n",
    "                           'y_true':y_true, \n",
    "                           'y_pred': y_pred},\n",
    "                         )\n",
    "evaluation.to_csv(\"./finetune/co_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da8e1d-2fbe-49b3-aa5b-72ea608245e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Fine-tuning for other rating categories (experiment) <a name=\"all_ft\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1497f900-0881-4c33-8858-cebe711a6785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cb', 'sm', 'cv', 'di', 'wlb']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = list(data_dict['train'].keys())\n",
    "cats.remove('co')\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6391f3d-16f3-4e1f-b07d-d8f310b1066e",
   "metadata": {},
   "source": [
    "Do not use this for fine-tuning,<br>\n",
    "Running this in a loop likely is causing VRAM not to be cleared, making training time untenable.<br>\n",
    "Use the batch file that calls the py script in a loop instead (or make your own shell script if on unix based systems)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b65ad1cb-a7e6-432e-a343-c329e4bc9e9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "for cat in cats:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config, \n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    max_seq_length = 512 #2048\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output_dir=f\"./trained_weights/{cat}\"\n",
    "    training_arguments.output_dir=output_dir    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        train_dataset=data_dict['train'][cat],\n",
    "        #eval_dataset=eval_data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=False,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False,\n",
    "            \"append_concat_token\": False,\n",
    "        },\n",
    "        #compute_metrics=compute_metrics,\n",
    "        #callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c93425-01fd-4dcd-bca6-fc4945822103",
   "metadata": {},
   "source": [
    "---\n",
    "# Load saved models and evaluate (experiment) <a name=\"all_eval\"></a>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "722de9a8-9432-485a-8b89-f451ee7b8bd4",
   "metadata": {},
   "source": [
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, './trained_weights/co')\n",
    "model = ft_model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./trained_weights/co', max_seq_length=max_seq_length)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186ae8c-c0c3-46fe-9046-6567f8311a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba8fe812d844ac7b4e385b73e24e43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [05:48<00:00,  4.31it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7020343207d419c9062ea9d3ee56ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                           | 48/1500 [02:37<1:55:02,  4.75s/it]"
     ]
    }
   ],
   "source": [
    "cats = list(data_dict['train'].keys())\n",
    "\n",
    "for cat in cats:\n",
    "    model_path = f'./trained_weights/{cat}'\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=device,\n",
    "        torch_dtype=compute_dtype,\n",
    "        quantization_config=bnb_config, \n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    max_seq_length = 512 #2048    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, max_seq_length=max_seq_length)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    to_predict = data_dict['test'][cat].iloc[:]\n",
    "    y_pred, _ = predict(to_predict, model, tokenizer)\n",
    "    \n",
    "    y_true = data_dict['y_true'][cat][:]\n",
    "    evaluation = pd.DataFrame({'prompt': data_dict['test'][cat]['prompt'], \n",
    "                               'y_true':y_true, \n",
    "                               'y_pred': y_pred,\n",
    "                               'base_pred':base_eval[cat]['y_pred']},\n",
    "                             )\n",
    "    evaluation.to_csv(f\"./finetune/{cat}_test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de39aa-430c-4c25-a7b6-74164e45f152",
   "metadata": {},
   "source": [
    "---\n",
    "# Create firm summary dataset for prediction <a name=\"createdata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e53ebd7-aed8-42f3-82ec-b91ea036649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = pd.read_csv('1_summary_reviews.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d33f0273-6a09-4c49-b777-fa02fd33fa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>firm</th>\n",
       "      <th>pros: career opportunities</th>\n",
       "      <th>pros: compensation and benefits</th>\n",
       "      <th>pros: senior management</th>\n",
       "      <th>pros: work life balance</th>\n",
       "      <th>pros: culture and values</th>\n",
       "      <th>pros: diversity and inclusion</th>\n",
       "      <th>cons: career opportunities</th>\n",
       "      <th>cons: compensation and benefits</th>\n",
       "      <th>cons: senior management</th>\n",
       "      <th>...</th>\n",
       "      <th>cons: culture and values</th>\n",
       "      <th>cons: diversity and inclusion</th>\n",
       "      <th>index</th>\n",
       "      <th>opportunities</th>\n",
       "      <th>compensation</th>\n",
       "      <th>management</th>\n",
       "      <th>worklife_balance</th>\n",
       "      <th>culture</th>\n",
       "      <th>diversity</th>\n",
       "      <th>kmeans_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMR</td>\n",
       "      <td>AMR has multiple locations throughout the glob...</td>\n",
       "      <td>Good pay with ability to make a lot more with ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Only work 3 days a week. Every other 3 day wee...</td>\n",
       "      <td>Great company culture, fast moving industry, o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You will get called in a lot. Mandatory overti...</td>\n",
       "      <td>Have to work a lot of O/T because always short...</td>\n",
       "      <td>Management has no idea what they’re doing or h...</td>\n",
       "      <td>...</td>\n",
       "      <td>Very bad upper management, Lack of culture Shi...</td>\n",
       "      <td>-Two Separate Division with Different Contract...</td>\n",
       "      <td>877</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  firm                         pros: career opportunities  \\\n",
       "0  AMR  AMR has multiple locations throughout the glob...   \n",
       "\n",
       "                     pros: compensation and benefits pros: senior management  \\\n",
       "0  Good pay with ability to make a lot more with ...                     NaN   \n",
       "\n",
       "                             pros: work life balance  \\\n",
       "0  Only work 3 days a week. Every other 3 day wee...   \n",
       "\n",
       "                            pros: culture and values  \\\n",
       "0  Great company culture, fast moving industry, o...   \n",
       "\n",
       "  pros: diversity and inclusion  \\\n",
       "0                           NaN   \n",
       "\n",
       "                          cons: career opportunities  \\\n",
       "0  You will get called in a lot. Mandatory overti...   \n",
       "\n",
       "                     cons: compensation and benefits  \\\n",
       "0  Have to work a lot of O/T because always short...   \n",
       "\n",
       "                             cons: senior management  ...  \\\n",
       "0  Management has no idea what they’re doing or h...  ...   \n",
       "\n",
       "                            cons: culture and values  \\\n",
       "0  Very bad upper management, Lack of culture Shi...   \n",
       "\n",
       "                       cons: diversity and inclusion index  opportunities  \\\n",
       "0  -Two Separate Division with Different Contract...   877           3.02   \n",
       "\n",
       "   compensation  management  worklife_balance  culture  diversity  \\\n",
       "0          2.82        2.61              2.64     2.69        3.3   \n",
       "\n",
       "   kmeans_labels  \n",
       "0              3  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51c822eb-ada5-451f-9131-99b12d6d1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pros=summ.iloc[:,1:7].astype(str).agg('\\n'.join,axis=1)\n",
    "cons=summ.iloc[:,7:13].astype(str).agg('\\n'.join,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1661c2e5-88e6-43a9-b003-3f6793261f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b31bd631-4cc6-44aa-b30c-97d0827597e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'pros: '+pros+'\\ncons: '+cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "56d744ce-d08e-42d5-8588-9e69df72288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(review,columns=['text']).to_csv('2_summ_text.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ftune]",
   "language": "python",
   "name": "conda-env-ftune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
